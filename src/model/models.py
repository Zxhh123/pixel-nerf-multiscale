"""
Main model implementation with Multi-View Attention and Smart Feature Fusion
Enhanced version with multi-scale features
"""
import torch
import torch.nn as nn
from .encoder import ImageEncoder
from .code import PositionalEncoding
from .model_util import make_encoder, make_mlp
import torch.autograd.profiler as profiler
from util import repeat_interleave
import os
import os.path as osp
import warnings


class PixelNeRFNet(torch.nn.Module):
    def __init__(self, conf, stop_encoder_grad=False):
        """
        :param conf PyHocon config subtree 'model'
        :param stop_encoder_grad: æ˜¯å¦åœæ­¢ encoder çš„æ¢¯åº¦ä¼ æ’­
        """
        super().__init__()

        # ========== ç¼–ç å™¨åˆå§‹åŒ– ==========
        self.encoder = make_encoder(conf["encoder"])
        self.use_encoder = conf.get_bool("use_encoder", True)
        self.use_xyz = conf.get_bool("use_xyz", False)
        assert self.use_encoder or self.use_xyz

        # ========== åŸºç¡€é…ç½® ==========
        self.normalize_z = conf.get_bool("normalize_z", True)
        self.stop_encoder_grad = stop_encoder_grad
        self.use_code = conf.get_bool("use_code", False)
        self.use_code_viewdirs = conf.get_bool("use_code_viewdirs", True)
        self.use_viewdirs = conf.get_bool("use_viewdirs", False)
        self.use_global_encoder = conf.get_bool("use_global_encoder", False)

        # ========== æ™ºèƒ½ç‰¹å¾èåˆé…ç½® ==========
        self.use_smart_fusion = conf.get_bool("use_smart_fusion", False)
        self.use_adaptive_sampling = conf.get_bool("use_adaptive_sampling", False)
        self.fusion_heads = conf.get_int("fusion_heads", 8)
        self.fusion_dropout = conf.get_float("fusion_dropout", 0.1)
        self.fusion_type = conf.get_string("fusion_type", "attention")
        self.use_cbam = conf.get_bool("use_cbam", True)
        self.quality_threshold = conf.get_float("quality_threshold", 0.3)

        # ========== ä¿®å¤ï¼šç»Ÿä¸€å¤„ç† encoder.latent_size ==========
        encoder_latent_size = self.encoder.latent_size
        if isinstance(encoder_latent_size, (list, tuple)):
            # âœ… å¤šå°ºåº¦ç‰¹å¾ï¼šæ±‚å’Œå¾—åˆ°æ€»ç»´åº¦
            self.latent_size = sum(int(x) for x in encoder_latent_size)
            self.is_multi_scale = True
            self.layer_dims = [int(x) for x in encoder_latent_size]
            print(f"âœ… Multi-scale encoder detected:")
            print(f"   - Layer sizes: {self.layer_dims}")
            print(f"   - Total latent size: {self.latent_size}")
        else:
            # âœ… å•å°ºåº¦ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨
            self.latent_size = int(encoder_latent_size)
            self.is_multi_scale = False
            self.layer_dims = [self.latent_size]
            print(f"âœ… Single-scale encoder:")
            print(f"   - Latent size: {self.latent_size}")

        # ========== åˆå§‹åŒ–æ™ºèƒ½ç‰¹å¾èåˆæ¨¡å— ==========
        if self.use_smart_fusion:
            try:
                from .feature_fusion import SmartFeatureFusion

                # å¦‚æœæ˜¯å¤šå°ºåº¦ï¼Œä½¿ç”¨èåˆæ¨¡å—
                if self.is_multi_scale:
                    self.feature_fusion = SmartFeatureFusion(
                        layer_dims=self.layer_dims,
                        output_dim=512,  # èåˆåçš„è¾“å‡ºç»´åº¦
                        use_attention=(self.fusion_type == "attention"),
                        dropout=self.fusion_dropout,
                        num_heads=self.fusion_heads,
                        use_cbam=self.use_cbam
                    )
                    # æ›´æ–° latent_size ä¸ºèåˆåçš„ç»´åº¦
                    self.latent_size = 512
                    print(f"âœ… Smart Feature Fusion enabled:")
                    print(f"   - Fusion type: {self.fusion_type}")
                    print(f"   - Fusion heads: {self.fusion_heads}")
                    print(f"   - CBAM: {'âœ…' if self.use_cbam else 'âŒ'}")
                    print(f"   - Output dimension: {self.latent_size}")
                else:
                    print(f"âš ï¸  Smart fusion requested but encoder is single-scale")
                    self.use_smart_fusion = False

            except ImportError as e:
                print(f"âŒ Failed to import SmartFeatureFusion: {e}")
                print(f"âš ï¸  Falling back to basic multi-scale concatenation")
                self.use_smart_fusion = False

        # ========== ä½ç½®ç¼–ç  ==========
        d_latent = 0
        d_in = 3  # xyz åæ ‡

        if self.use_code:
            num_freqs = conf.get_int("code.num_freqs", 6)
            freq_factor = conf.get_float("code.freq_factor", 1.5)
            include_input = conf.get_bool("code.include_input", True)
            self.code = PositionalEncoding.from_conf(
                num_freqs, freq_factor=freq_factor, include_input=include_input
            )
            d_in = self.code.d_out
            print(f"âœ… Positional encoding for xyz: {d_in} dims")

        # è§†è§’æ–¹å‘ç¼–ç 
        if self.use_viewdirs:
            if self.use_code_viewdirs:
                num_freqs_viewdirs = conf.get_int("code_viewdirs.num_freqs", 4)
                freq_factor_viewdirs = conf.get_float("code_viewdirs.freq_factor", 1.5)
                include_input_viewdirs = conf.get_bool("code_viewdirs.include_input", True)
                self.code_viewdirs = PositionalEncoding.from_conf(
                    num_freqs_viewdirs,
                    freq_factor=freq_factor_viewdirs,
                    include_input=include_input_viewdirs
                )
                d_latent = self.code_viewdirs.d_out
                print(f"âœ… Positional encoding for viewdirs: {d_latent} dims")
            else:
                d_latent = 3
                print(f"âœ… Raw viewdirs: {d_latent} dims")

        # ========== MLP è¾“å…¥ç»´åº¦è®¡ç®— ==========
        if self.use_encoder:
            d_in = self.latent_size + d_in  # ç‰¹å¾ + xyzç¼–ç 

        print(f"\nğŸ“Š MLP Input Configuration:")
        print(f"   - Feature dimension: {self.latent_size}")
        print(f"   - XYZ dimension: {d_in - self.latent_size}")
        print(f"   - Viewdir dimension: {d_latent}")
        print(f"   - Total input dimension: {d_in}")

        # ========== MLP è§£ç å™¨ ==========
        self.mlp_coarse = make_mlp(conf["mlp_coarse"], d_in, d_latent=d_latent)
        self.mlp_fine = make_mlp(
            conf["mlp_fine"], d_in, d_latent=d_latent, allow_empty=True
        )

        # å¦‚æœæ²¡æœ‰ fine ç½‘ç»œï¼Œä½¿ç”¨ coarse ç½‘ç»œ
        if self.mlp_fine is None:
            self.mlp_fine = self.mlp_coarse
            print("âš ï¸  No separate fine MLP, using coarse MLP for both")

        # è¾“å‡ºç»´åº¦
        self.d_in = d_in
        self.d_out = 4  # RGB + density
        self.d_latent = d_latent

        # ========== å…¨å±€ç‰¹å¾ç¼–ç å™¨ï¼ˆå¯é€‰ï¼‰ ==========
        if self.use_global_encoder:
            self.global_encoder = nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(self.latent_size, 256),
                nn.ReLU(inplace=True),
                nn.Linear(256, 256)
            )
            print("âœ… Global encoder enabled")

        print(f"\nâœ… PixelNeRFNet initialized:")
        print(f"   - Input dimension: {d_in}")
        print(f"   - Latent dimension: {d_latent}")
        print(f"   - Output dimension: {self.d_out}")
        print(f"   - Use encoder: {self.use_encoder}")
        print(f"   - Use xyz: {self.use_xyz}")
        print(f"   - Use viewdirs: {self.use_viewdirs}")
        print(f"   - Smart fusion: {'âœ…' if self.use_smart_fusion else 'âŒ'}")
        print(f"   - Adaptive sampling: {'âœ…' if self.use_adaptive_sampling else 'âŒ'}")

    def encode(self, images, poses, focal, z_bounds=None, c=None):
        """
        ç¼–ç è¾“å…¥å›¾åƒ
        :param images (NS, 3, H, W) è¾“å…¥å›¾åƒ
        :param poses (NS, 4, 4) ç›¸æœºä½å§¿
        :param focal (NS,) æˆ– (NS, 2) ç„¦è·
        :param z_bounds (NS, 2) æ·±åº¦è¾¹ç•Œ
        :param c (NS,) å¯é€‰çš„ç±»åˆ«ç¼–ç 
        :return latent ç¼–ç åçš„ç‰¹å¾
        """
        if images.shape[0] == 0:
            return None

        # âœ… åœæ­¢æ¢¯åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.stop_encoder_grad:
            images = images.detach()

        # âœ… ç¼–ç å›¾åƒ
        with profiler.record_function("encoder_forward"):
            latent = self.encoder(images)

        # âœ… åº”ç”¨æ™ºèƒ½ç‰¹å¾èåˆï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_smart_fusion and self.is_multi_scale:
            with profiler.record_function("feature_fusion"):
                # latent æ˜¯å¤šå°ºåº¦ç‰¹å¾åˆ—è¡¨
                if isinstance(latent, list):
                    latent = self.feature_fusion(latent)  # è¿”å› (NS, output_dim, H, W)

        # âœ… ä¿å­˜ç¼–ç åçš„ç‰¹å¾å’Œç›¸æœºå‚æ•°
        self.latent = latent
        self.poses = poses
        self.focal = focal
        self.c = c
        self.z_bounds = z_bounds

        # âœ… è®¡ç®—å›¾åƒå°ºå¯¸
        if isinstance(latent, torch.Tensor):
            self.latent_scaling = images.shape[-1] / latent.shape[-1]
        else:
            # å¤šå°ºåº¦ç‰¹å¾ï¼Œä½¿ç”¨ç¬¬ä¸€å±‚çš„å°ºå¯¸
            self.latent_scaling = images.shape[-1] / latent[0].shape[-1]

        # âœ… å…¨å±€ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        if self.use_global_encoder:
            self.global_latent = self.global_encoder(latent)
        else:
            self.global_latent = None

        return latent

    def forward(self, xyz, coarse=True, viewdirs=None, far=False):
        """
        å‰å‘ä¼ æ’­
        :param xyz (SB, B, 3) 3D åæ ‡ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰
        :param coarse bool æ˜¯å¦ä½¿ç”¨ç²—ç½‘ç»œ
        :param viewdirs (SB, B, 3) è§†è§’æ–¹å‘
        :param far bool æ˜¯å¦æ˜¯è¿œè·ç¦»ç‚¹
        :return (SB, B, 4) RGB + density
        """
        with profiler.record_function("model_forward"):
            SB, B, _ = xyz.shape

            # âœ… ä»ç¼–ç å™¨è·å–åƒç´ å¯¹é½çš„ç‰¹å¾
            with profiler.record_function("encoder_index"):
                # å°†ä¸–ç•Œåæ ‡è½¬æ¢åˆ°ç›¸æœºåæ ‡
                xyz_rot = torch.matmul(self.poses[:, None, :3, :3], xyz.unsqueeze(-1))[..., 0]
                xyz_cam = xyz_rot + self.poses[:, None, :3, 3]

                # æŠ•å½±åˆ°å›¾åƒå¹³é¢
                if self.focal.shape[-1] == 2:
                    fx, fy = self.focal[..., 0], self.focal[..., 1]
                else:
                    fx = fy = self.focal

                uv = torch.stack([
                    xyz_cam[..., 0] / xyz_cam[..., 2] * fx[:, None],
                    xyz_cam[..., 1] / xyz_cam[..., 2] * fy[:, None]
                ], dim=-1)

                # å½’ä¸€åŒ–åˆ° [-1, 1]
                if isinstance(self.latent, torch.Tensor):
                    H, W = self.latent.shape[-2:]
                else:
                    H, W = self.latent[0].shape[-2:]

                uv = uv / torch.tensor([W / 2, H / 2], device=uv.device) - 1.0

                # é‡‡æ ·ç‰¹å¾
                if isinstance(self.latent, torch.Tensor):
                    # å•å°ºåº¦ç‰¹å¾
                    latent_feat = torch.nn.functional.grid_sample(
                        self.latent,
                        uv.view(SB, 1, B, 2),
                        align_corners=True,
                        mode='bilinear',
                        padding_mode='border'
                    )  # (SB, C, 1, B)
                    latent_feat = latent_feat.squeeze(2).transpose(1, 2)  # (SB, B, C)
                else:
                    # å¤šå°ºåº¦ç‰¹å¾ï¼ˆå·²èåˆï¼‰
                    latent_feat = torch.nn.functional.grid_sample(
                        self.latent,
                        uv.view(SB, 1, B, 2),
                        align_corners=True,
                        mode='bilinear',
                        padding_mode='border'
                    )
                    latent_feat = latent_feat.squeeze(2).transpose(1, 2)

            # âœ… æ„å»º MLP è¾“å…¥
            mlp_input = latent_feat

            # æ·»åŠ  xyz ç¼–ç 
            if self.use_xyz:
                if self.use_code:
                    xyz_encoded = self.code(xyz)
                else:
                    xyz_encoded = xyz
                mlp_input = torch.cat([mlp_input, xyz_encoded], dim=-1)

            # æ·»åŠ è§†è§’æ–¹å‘ç¼–ç 
            if self.use_viewdirs and viewdirs is not None:
                if self.use_code_viewdirs:
                    viewdirs_encoded = self.code_viewdirs(viewdirs)
                else:
                    viewdirs_encoded = viewdirs
                # viewdirs ä½œä¸º latent è¾“å…¥
                latent_input = viewdirs_encoded
            else:
                latent_input = None

            # âœ… MLP è§£ç 
            mlp = self.mlp_coarse if coarse else self.mlp_fine

            with profiler.record_function("mlp_forward"):
                if latent_input is not None:
                    mlp_output = mlp(mlp_input, combine_inner_dims=(1,), combine_index=mlp.d_latent, dim_size=B, latent=latent_input)
                else:
                    mlp_output = mlp(mlp_input, combine_inner_dims=(1,), combine_index=mlp.d_latent, dim_size=B)

            # âœ… è¾“å‡ºï¼šRGB + density
            return mlp_output

    def load_weights(self, args, opt_init=False, strict=True, device=None):
        """
        åŠ è½½é¢„è®­ç»ƒæƒé‡
        """
        if device is None:
            device = torch.device("cpu")

        # åŠ è½½æƒé‡æ–‡ä»¶
        if hasattr(args, 'resume') and args.resume and os.path.isfile(args.resume):
            print(f"âœ… Loading checkpoint from {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)

            # åŠ è½½æ¨¡å‹æƒé‡
            if "model_state_dict" in checkpoint:
                self.load_state_dict(checkpoint["model_state_dict"], strict=strict)
            elif "model" in checkpoint:
                self.load_state_dict(checkpoint["model"], strict=strict)
            else:
                self.load_state_dict(checkpoint, strict=strict)

            print("âœ… Checkpoint loaded successfully")

            return checkpoint
        else:
            if hasattr(args, 'resume') and args.resume:
                warnings.warn(f"âŒ Checkpoint file not found: {args.resume}")
            return None

    def save_weights(self, path, optimizer=None, epoch=None):
        """
        ä¿å­˜æ¨¡å‹æƒé‡
        """
        checkpoint = {
            "model_state_dict": self.state_dict(),
            "epoch": epoch,
        }
        if optimizer is not None:
            checkpoint["optimizer_state_dict"] = optimizer.state_dict()

        torch.save(checkpoint, path)
        print(f"âœ… Checkpoint saved to {path}")


def make_model(conf):
    """
    åˆ›å»º PixelNeRF æ¨¡å‹
    """
    return PixelNeRFNet(conf)
