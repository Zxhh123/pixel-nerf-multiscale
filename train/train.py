# Training to a set of multiple objects (e.g. ShapeNet or DTU)
# tensorboard logs available in logs/<expname>

import sys
import os

sys.path.insert(
    0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "src"))
)

import warnings
import trainlib
from model import make_model, loss
from render import NeRFRenderer
from data import get_split_dataset
import util
import numpy as np
import torch.nn.functional as F
import torch
from dotmap import DotMap

# âœ… æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
from torch.amp import autocast, GradScaler


def extra_args(parser):
    parser.add_argument(
        "--batch_size", "-B", type=int, default=4, help="Object batch size ('SB')"
    )
    parser.add_argument(
        "--nviews",
        "-V",
        type=str,
        default="1",
        help="Number of source views (multiview); put multiple (space delim) to pick randomly per batch ('NV')",
    )
    parser.add_argument(
        "--freeze_enc",
        action="store_true",
        default=None,
        help="Freeze encoder weights and only train MLP",
    )
    parser.add_argument(
        "--no_bbox_step",
        type=int,
        default=100000,
        help="Step to stop using bbox sampling",
    )
    parser.add_argument(
        "--fixed_test",
        action="store_true",
        default=None,
        help="Use fixed test views",
    )
    # âœ… æ··åˆç²¾åº¦è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--use_amp",
        action="store_true",
        default=True,
        help="Use automatic mixed precision training",
    )
    parser.add_argument(
        "--no_amp",
        action="store_true",
        default=False,
        help="Disable automatic mixed precision training",
    )
    # âœ… æ¢¯åº¦æ£€æŸ¥å‚æ•°
    parser.add_argument(
        "--check_gradients",
        action="store_true",
        default=False,
        help="Enable gradient checking and clipping",
    )
    parser.add_argument(
        "--grad_clip",
        type=float,
        default=1.0,
        help="Gradient clipping threshold",
    )
    return parser


args, conf = util.args.parse_args(extra_args, training=True, default_ray_batch_size=128)
device = util.get_cuda(args.gpu_id[0])

# âœ… å¤„ç† AMP æ ‡å¿—
if args.no_amp:
    args.use_amp = False

print("\n" + "=" * 80)
print("ğŸš€ PIXELNERF TRAINING - ENHANCED VERSION")
print("=" * 80)
print(f"ğŸ“ Device: {device}")
print(f"ğŸ“¦ Batch size: {args.batch_size}")
print(f"ğŸ‘ï¸  Number of views: {args.nviews}")
print(f"ğŸ¯ Ray batch size: {args.ray_batch_size}")
print(f"âš¡ Mixed precision (AMP): {'âœ… Enabled' if args.use_amp else 'âŒ Disabled'}")
print(f"â„ï¸  Freeze encoder: {'âœ… Yes' if args.freeze_enc else 'âŒ No'}")
print(f"âœ‚ï¸  Gradient clipping: {'âœ… Enabled' if args.check_gradients else 'âŒ Disabled'} (threshold: {args.grad_clip})")
print("=" * 80 + "\n")

# ========== åŠ è½½æ•°æ®é›† ==========
print("ğŸ“‚ Loading datasets...")
dset, val_dset, _ = get_split_dataset(args.dataset_format, args.datadir)
print(f"âœ… Dataset loaded:")
print(f"   - Train samples: {len(dset)}")
print(f"   - Val samples: {len(val_dset) if val_dset is not None else 0}")
print(f"   - z_near: {dset.z_near}, z_far: {dset.z_far}")
print(f"   - lindisp: {dset.lindisp}")

# ========== åˆ›å»ºæ¨¡å‹ ==========
print("\nğŸ—ï¸  Creating model with enhanced features...")
net = make_model(conf["model"]).to(device=device)
net.stop_encoder_grad = args.freeze_enc

if args.freeze_enc:
    print("â„ï¸  Encoder frozen (fine-tuning mode)")
    net.encoder.eval()
    for param in net.encoder.parameters():
        param.requires_grad = False

# âœ… æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
print(f"\nğŸ“Š Model Configuration:")
print(f"   - Encoder type: {net.encoder.__class__.__name__}")
print(f"   - Latent size: {net.latent_size}")
print(f"   - Use encoder: {net.use_encoder}")
print(f"   - Use xyz: {net.use_xyz}")

# âœ… æ‰“å°æ–°å¢åŠŸèƒ½çŠ¶æ€
if hasattr(net, 'use_smart_fusion'):
    print(f"   - Smart fusion: {'âœ… Enabled' if net.use_smart_fusion else 'âŒ Disabled'}")
    if net.use_smart_fusion:
        print(f"      - Fusion type: {net.fusion_type if hasattr(net, 'fusion_type') else 'attention'}")
        print(f"      - Fusion heads: {net.fusion_heads}")
        print(f"      - CBAM: {'âœ…' if net.use_cbam else 'âŒ'}")

if hasattr(net, 'use_adaptive_sampling'):
    print(f"   - Adaptive sampling: {'âœ… Enabled' if net.use_adaptive_sampling else 'âŒ Disabled'}")
    if net.use_adaptive_sampling:
        print(f"      - Quality threshold: {net.quality_threshold}")

if hasattr(net, 'use_attention'):
    print(f"   - Legacy attention: {'âœ… Enabled' if net.use_attention else 'âŒ Disabled'}")
    if net.use_attention:
        print(f"      - Attention heads: {net.attention_heads}")

# âœ… æ‰“å°ç¼–ç å™¨ä¿¡æ¯
if hasattr(net.encoder, 'use_multi_scale'):
    print(f"   - Multi-scale encoder: {'âœ… Enabled' if net.encoder.use_multi_scale else 'âŒ Disabled'}")
    if net.encoder.use_multi_scale:
        print(f"      - Feature scales: {net.encoder.latent_size}")

# ========== åˆ›å»ºæ¸²æŸ“å™¨ ==========
print("\nğŸ¨ Creating renderer...")
renderer = NeRFRenderer.from_conf(
    conf["renderer"],
    lindisp=dset.lindisp,
).to(device=device)

# ========== å¹¶è¡ŒåŒ– ==========
print(f"\nâš¡ Setting up parallelization on GPUs: {args.gpu_id}")
render_par = renderer.bind_parallel(net, args.gpu_id).eval()

nviews = list(map(int, args.nviews.split()))
print(f"âœ… Multi-view setup: {nviews} views per batch")


class PixelNeRFTrainer(trainlib.Trainer):
    def __init__(self):
        # âœ… ä¼ é€’ use_amp å‚æ•°åˆ°çˆ¶ç±»
        super().__init__(
            net,
            dset,
            val_dset,
            args,
            conf["train"],
            device=device,
            use_amp=args.use_amp
        )

        self.renderer_state_path = "%s/%s/_renderer" % (
            self.args.checkpoints_path,
            self.args.name,
        )

        # ========== æŸå¤±å‡½æ•°é…ç½® ==========
        self.lambda_coarse = conf.get_float("loss.lambda_coarse")
        self.lambda_fine = conf.get_float("loss.lambda_fine", 1.0)
        print(f"\nğŸ“Š Loss configuration:")
        print(f"   - Lambda coarse: {self.lambda_coarse}")
        print(f"   - Lambda fine: {self.lambda_fine}")

        self.rgb_coarse_crit = loss.get_rgb_loss(conf["loss.rgb"], True)
        fine_loss_conf = conf["loss.rgb"]
        if "rgb_fine" in conf["loss"]:
            print("   - Using separate fine loss configuration")
            fine_loss_conf = conf["loss.rgb_fine"]
        self.rgb_fine_crit = loss.get_rgb_loss(fine_loss_conf, False)

        # ========== æ¢å¤æ¸²æŸ“å™¨çŠ¶æ€ ==========
        if args.resume:
            if os.path.exists(self.renderer_state_path):
                print(f"ğŸ“¥ Loading renderer state from {self.renderer_state_path}")
                renderer.load_state_dict(
                    torch.load(self.renderer_state_path, map_location=device)
                )

        # ========== æ·±åº¦èŒƒå›´ ==========
        self.z_near = dset.z_near
        self.z_far = dset.z_far

        # ========== BBox é‡‡æ · ==========
        self.use_bbox = args.no_bbox_step > 0
        if self.use_bbox:
            print(f"ğŸ“¦ BBox sampling enabled (will disable at step {args.no_bbox_step})")

        # ========== æ··åˆç²¾åº¦è®­ç»ƒ ==========
        if self.use_amp:
            print("âœ… Mixed Precision Training (AMP) enabled")
            if not hasattr(self, 'scaler'):
                self.scaler = GradScaler('cuda')
                print("   - GradScaler initialized")
        else:
            print("âŒ Mixed Precision Training (AMP) disabled")

        # ========== è®­ç»ƒç›‘æ§ ==========
        self.global_step = 0
        self.check_gradients = args.check_gradients
        self.grad_clip = args.grad_clip

        if self.check_gradients:
            print(f"âœ… Gradient checking enabled (clip threshold: {self.grad_clip})")

        # ========== ç»Ÿè®¡ä¿¡æ¯ ==========
        self.loss_history = []
        self.psnr_history = []
        self.best_psnr = 0.0

        print("\n" + "=" * 80)
        print("âœ… Trainer initialization complete!")
        print("=" * 80 + "\n")

    def post_batch(self, epoch, batch):
        """Batch ç»“æŸåçš„å›è°ƒ"""
        renderer.sched_step(args.batch_size)

    def extra_save_state(self):
        """ä¿å­˜é¢å¤–çš„çŠ¶æ€"""
        torch.save(renderer.state_dict(), self.renderer_state_path)

    def calc_losses(self, data, is_train=True, global_step=0):
        """
        è®¡ç®—æŸå¤±å‡½æ•°

        âœ… é€‚é…æ–°çš„ encoder å’Œ feature fusion
        """
        if "images" not in data:
            return {}

        all_images = data["images"].to(device=device)  # (SB, NV, 3, H, W)

        SB, NV, _, H, W = all_images.shape
        all_poses = data["poses"].to(device=device)  # (SB, NV, 4, 4)
        all_bboxes = data.get("bbox")  # (SB, NV, 4)  cmin rmin cmax rmax
        all_focals = data["focal"]  # (SB)
        all_c = data.get("c")  # (SB)

        # ========== BBox é‡‡æ ·æ§åˆ¶ ==========
        if self.use_bbox and global_step >= args.no_bbox_step:
            self.use_bbox = False
            print(f"\nğŸ“¦ Stopped using bbox sampling @ step {global_step}\n")

        if not is_train or not self.use_bbox:
            all_bboxes = None

        # ========== å‡†å¤‡æ•°æ® ==========
        all_rgb_gt = []
        all_rays = []

        curr_nviews = nviews[torch.randint(0, len(nviews), ()).item()]
        if curr_nviews == 1:
            image_ord = torch.randint(0, NV, (SB, 1))
        else:
            image_ord = torch.empty((SB, curr_nviews), dtype=torch.long)

        for obj_idx in range(SB):
            if all_bboxes is not None:
                bboxes = all_bboxes[obj_idx]
            images = all_images[obj_idx]  # (NV, 3, H, W)
            poses = all_poses[obj_idx]  # (NV, 4, 4)
            focal = all_focals[obj_idx]
            c = None
            if "c" in data:
                c = data["c"][obj_idx]

            if curr_nviews > 1:
                image_ord[obj_idx] = torch.from_numpy(
                    np.random.choice(NV, curr_nviews, replace=False)
                )

            images_0to1 = images * 0.5 + 0.5

            cam_rays = util.gen_rays(
                poses, W, H, focal, self.z_near, self.z_far, c=c
            )  # (NV, H, W, 8)
            rgb_gt_all = images_0to1
            rgb_gt_all = (
                rgb_gt_all.permute(0, 2, 3, 1).contiguous().reshape(-1, 3)
            )  # (NV, H, W, 3)

            if all_bboxes is not None:
                pix = util.bbox_sample(bboxes, args.ray_batch_size)
                pix_inds = pix[..., 0] * H * W + pix[..., 1] * W + pix[..., 2]
            else:
                pix_inds = torch.randint(0, NV * H * W, (args.ray_batch_size,))

            rgb_gt = rgb_gt_all[pix_inds]  # (ray_batch_size, 3)
            rays = cam_rays.view(-1, cam_rays.shape[-1])[pix_inds].to(
                device=device
            )  # (ray_batch_size, 8)

            all_rgb_gt.append(rgb_gt)
            all_rays.append(rays)

        all_rgb_gt = torch.stack(all_rgb_gt)  # (SB, ray_batch_size, 3)
        all_rays = torch.stack(all_rays)  # (SB, ray_batch_size, 8)

        image_ord = image_ord.to(device)
        src_images = util.batched_index_select_nd(
            all_images, image_ord
        )  # (SB, NS, 3, H, W)
        src_poses = util.batched_index_select_nd(all_poses, image_ord)  # (SB, NS, 4, 4)

        all_bboxes = all_poses = all_images = None

        # ========== ç¼–ç ï¼ˆâœ… ä¼šè‡ªåŠ¨ä½¿ç”¨æ–°çš„ feature fusionï¼‰ ==========
        net.encode(
            src_images,
            src_poses,
            all_focals.to(device=device),
            c=all_c.to(device=device) if all_c is not None else None,
        )

        # ========== æ¸²æŸ“ ==========
        render_dict = DotMap(render_par(all_rays, want_weights=True))
        coarse = render_dict.coarse
        fine = render_dict.fine
        using_fine = len(fine) > 0

        # ========== è®¡ç®—æŸå¤± ==========
        loss_dict = {}

        rgb_loss = self.rgb_coarse_crit(coarse.rgb, all_rgb_gt)
        loss_dict["rc"] = rgb_loss.item() * self.lambda_coarse

        if using_fine:
            fine_loss = self.rgb_fine_crit(fine.rgb, all_rgb_gt)
            rgb_loss = rgb_loss * self.lambda_coarse + fine_loss * self.lambda_fine
            loss_dict["rf"] = fine_loss.item() * self.lambda_fine

        loss = rgb_loss

        # ========== è®¡ç®— PSNR ==========
        if using_fine:
            rgb_pred = fine.rgb
        else:
            rgb_pred = coarse.rgb

        mse = F.mse_loss(rgb_pred, all_rgb_gt)
        psnr = -10 * torch.log10(mse)
        loss_dict["psnr"] = psnr.item()

        if is_train:
            loss_dict["loss"] = loss

        loss_dict["t"] = loss.item()

        return loss_dict

    def train_step(self, data, global_step):
        """
        è®­ç»ƒæ­¥éª¤

        âœ… ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.optimizer.zero_grad()

        # ========== æ··åˆç²¾åº¦è®­ç»ƒ ==========
        if self.use_amp:
            with autocast('cuda'):
                loss_dict = self.calc_losses(data, is_train=True, global_step=global_step)
                loss = loss_dict["loss"]

            # åå‘ä¼ æ’­ï¼ˆä½¿ç”¨ scalerï¼‰
            self.scaler.scale(loss).backward()

            # âœ… æ¢¯åº¦æ£€æŸ¥å’Œè£å‰ª
            if self.check_gradients:
                self.scaler.unscale_(self.optimizer)
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    net.parameters(),
                    self.grad_clip
                )
                if global_step % 100 == 0:
                    print(f"   ğŸ“Š Step {global_step}: grad_norm={grad_norm:.4f}")

            self.scaler.step(self.optimizer)
            self.scaler.update()

        else:
            # ========== åŸå§‹è®­ç»ƒæµç¨‹ ==========
            loss_dict = self.calc_losses(data, is_train=True, global_step=global_step)
            loss = loss_dict["loss"]
            loss.backward()

            # âœ… æ¢¯åº¦æ£€æŸ¥å’Œè£å‰ª
            if self.check_gradients:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    net.parameters(),
                    self.grad_clip
                )
                if global_step % 100 == 0:
                    print(f"   ğŸ“Š Step {global_step}: grad_norm={grad_norm:.4f}")

            self.optimizer.step()

        # ========== è®°å½•å†å² ==========
        self.loss_history.append(loss_dict["t"])
        self.psnr_history.append(loss_dict.get("psnr", 0))

        # ========== æ›´æ–°å…¨å±€æ­¥æ•° ==========
        self.global_step = global_step

        # ========== å®šæœŸæ‰“å° ==========
        if global_step % 50 == 0:
            print(f"   Step {global_step}: loss={loss_dict['t']:.4f}, psnr={loss_dict.get('psnr', 0):.2f} dB")

        return loss_dict

    def eval_step(self, data, global_step):
        """
        éªŒè¯æ­¥éª¤

        âœ… éªŒè¯æ—¶ä¹Ÿä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ
        """
        renderer.eval()

        if self.use_amp:
            with torch.no_grad():
                with autocast('cuda'):
                    losses = self.calc_losses(data, is_train=False, global_step=global_step)
        else:
            with torch.no_grad():
                losses = self.calc_losses(data, is_train=False, global_step=global_step)

        renderer.train()
        return losses

    def vis_step(self, data, global_step, idx=None):
        """
        å¯è§†åŒ–æ­¥éª¤

        âœ… é€‚é…æ–°çš„ç¼–ç å™¨
        """
        if "images" not in data:
            return {}

        if idx is None:
            batch_idx = np.random.randint(0, data["images"].shape[0])
        else:
            batch_idx = idx

        images = data["images"][batch_idx].to(device=device)  # (NV, 3, H, W)
        poses = data["poses"][batch_idx].to(device=device)  # (NV, 4, 4)
        focal = data["focal"][batch_idx: batch_idx + 1]  # (1)
        c = data.get("c")
        if c is not None:
            c = c[batch_idx: batch_idx + 1]  # (1)

        NV, _, H, W = images.shape
        cam_rays = util.gen_rays(
            poses, W, H, focal, self.z_near, self.z_far, c=c
        )  # (NV, H, W, 8)
        images_0to1 = images * 0.5 + 0.5  # (NV, 3, H, W)

        curr_nviews = nviews[torch.randint(0, len(nviews), (1,)).item()]
        views_src = np.sort(np.random.choice(NV, curr_nviews, replace=False))
        view_dest = np.random.randint(0, NV - curr_nviews)
        for vs in range(curr_nviews):
            view_dest += view_dest >= views_src[vs]
        views_src = torch.from_numpy(views_src)

        # ========== è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ ==========
        renderer.eval()
        source_views = (
            images_0to1[views_src]
            .permute(0, 2, 3, 1)
            .cpu()
            .numpy()
            .reshape(-1, H, W, 3)
        )

        gt = images_0to1[view_dest].permute(1, 2, 0).cpu().numpy().reshape(H, W, 3)

        with torch.no_grad():
            test_rays = cam_rays[view_dest]  # (H, W, 8)
            test_images = images[views_src]  # (NS, 3, H, W)

            # âœ… ç¼–ç ï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨æ–°çš„ feature fusionï¼‰
            net.encode(
                test_images.unsqueeze(0),
                poses[views_src].unsqueeze(0),
                focal.to(device=device),
                c=c.to(device=device) if c is not None else None,
            )
            test_rays = test_rays.reshape(1, H * W, -1)

            # âœ… ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†
            if self.use_amp:
                with autocast('cuda'):
                    render_dict = DotMap(render_par(test_rays, want_weights=True))
            else:
                render_dict = DotMap(render_par(test_rays, want_weights=True))

            coarse = render_dict.coarse
            fine = render_dict.fine

            using_fine = len(fine) > 0

            alpha_coarse_np = coarse.weights[0].sum(dim=-1).cpu().numpy().reshape(H, W)
            rgb_coarse_np = coarse.rgb[0].cpu().numpy().reshape(H, W, 3)
            depth_coarse_np = coarse.depth[0].cpu().numpy().reshape(H, W)

            if using_fine:
                alpha_fine_np = fine.weights[0].sum(dim=1).cpu().numpy().reshape(H, W)
                depth_fine_np = fine.depth[0].cpu().numpy().reshape(H, W)
                rgb_fine_np = fine.rgb[0].cpu().numpy().reshape(H, W, 3)

        print(f"Coarse: rgb [{rgb_coarse_np.min():.3f}, {rgb_coarse_np.max():.3f}], "
              f"alpha [{alpha_coarse_np.min():.3f}, {alpha_coarse_np.max():.3f}]")

        alpha_coarse_cmap = util.cmap(alpha_coarse_np) / 255
        depth_coarse_cmap = util.cmap(depth_coarse_np) / 255
        vis_list = [
            *source_views,
            gt,
            depth_coarse_cmap,
            rgb_coarse_np,
            alpha_coarse_cmap,
        ]

        vis_coarse = np.hstack(vis_list)
        vis = vis_coarse

        if using_fine:
            print(f"Fine: rgb [{rgb_fine_np.min():.3f}, {rgb_fine_np.max():.3f}], "
                  f"alpha [{alpha_fine_np.min():.3f}, {alpha_fine_np.max():.3f}]")
            depth_fine_cmap = util.cmap(depth_fine_np) / 255
            alpha_fine_cmap = util.cmap(alpha_fine_np) / 255
            vis_list = [
                *source_views,
                gt,
                depth_fine_cmap,
                rgb_fine_np,
                alpha_fine_cmap,
            ]

            vis_fine = np.hstack(vis_list)
            vis = np.vstack((vis_coarse, vis_fine))
            rgb_psnr = rgb_fine_np
        else:
            rgb_psnr = rgb_coarse_np

        psnr = util.psnr(rgb_psnr, gt)
        vals = {"psnr": psnr}
        print(f"Visualization PSNR: {psnr:.2f} dB")

        # âœ… æ›´æ–°æœ€ä½³ PSNR
        if psnr > self.best_psnr:
            self.best_psnr = psnr
            print(f"ğŸ‰ New best PSNR: {psnr:.2f} dB")

        # ========== æ¢å¤è®­ç»ƒæ¨¡å¼ ==========
        renderer.train()
        return vis, vals

    def post_epoch(self, epoch):
        """
        Epoch ç»“æŸåçš„å›è°ƒ
        """
        # ========== æ‰“å°ç»Ÿè®¡ä¿¡æ¯ ==========
        if len(self.loss_history) > 0:
            avg_loss = np.mean(self.loss_history[-100:])
            avg_psnr = np.mean(self.psnr_history[-100:])
            print(f"\nğŸ“Š Epoch {epoch} Summary:")
            print(f"   - Average loss (last 100 steps): {avg_loss:.4f}")
            print(f"   - Average PSNR (last 100 steps): {avg_psnr:.2f} dB")
            print(f"   - Best PSNR so far: {self.best_psnr:.2f} dB")
            print(f"   - Total steps: {self.global_step}")


# âœ… åˆ›å»ºè®­ç»ƒå™¨
print("\nğŸ¯ Creating trainer...")
trainer = PixelNeRFTrainer()
print("âœ… Trainer created successfully\n")

if __name__ == '__main__':
    # âœ… å¼€å§‹è®­ç»ƒ
    print("=" * 80)
    print("ğŸš€ STARTING TRAINING")
    print("=" * 80 + "\n")

    try:
        trainer.start()

        print("\n" + "=" * 80)
        print("ğŸ‰ TRAINING COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print(f"ğŸ“Š Final Statistics:")
        print(f"   - Total steps: {trainer.global_step}")
        print(f"   - Best PSNR: {trainer.best_psnr:.2f} dB")
        print("=" * 80 + "\n")

    except KeyboardInterrupt:
        print("\n" + "=" * 80)
        print("âš ï¸  TRAINING INTERRUPTED BY USER")
        print("=" * 80)
        print(f"ğŸ“Š Statistics at interruption:")
        print(f"   - Steps completed: {trainer.global_step}")
        print(f"   - Best PSNR: {trainer.best_psnr:.2f} dB")
        print("=" * 80 + "\n")

    except Exception as e:
        print("\n" + "=" * 80)
        print("âŒ TRAINING FAILED!")
        print("=" * 80)
        print(f"Error: {e}")
        import traceback

        traceback.print_exc()
        print("=" * 80 + "\n")

    finally:
        # âœ… ä¿å­˜æœ€ç»ˆçŠ¶æ€
        if hasattr(trainer, 'extra_save_state'):
            try:
                trainer.extra_save_state()
                print("ğŸ’¾ Final state saved successfully\n")
            except Exception as e:
                print(f"âš ï¸  Warning: Failed to save final state: {e}\n")
