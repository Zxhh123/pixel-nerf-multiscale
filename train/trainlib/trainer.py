import os.path
import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import tqdm
import warnings
import time
import glob  # âœ… æ·»åŠ  glob ç”¨äºæ¸…ç†æ—§ checkpoint

# âœ… æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
from torch.cuda.amp import autocast, GradScaler


def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰ collate å‡½æ•°ï¼Œå¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    1. è¿‡æ»¤æ‰ None æ ·æœ¬
    2. å¤„ç†ä¸åŒæ•°é‡å›¾ç‰‡çš„æ ·æœ¬ï¼ˆç»Ÿä¸€è£å‰ªæ‰€æœ‰ç›¸å…³å­—æ®µï¼‰
    3. å®‰å…¨å¤„ç†æ ‡é‡ tensor å’Œç©ºç»´åº¦

    âœ… ä¿®å¤ï¼šåˆ›å»ºæ–°çš„ tensor å‰¯æœ¬ï¼Œé¿å… "storage not resizable" é”™è¯¯
    âœ… ä¿®å¤ï¼šå®‰å…¨æ£€æŸ¥ tensor ç»´åº¦ï¼Œé¿å… IndexError
    """
    import warnings
    import torch

    # è¿‡æ»¤æ‰ Noneï¼ˆè·³è¿‡çš„æ ·æœ¬ï¼‰
    batch = [item for item in batch if item is not None]

    if len(batch) == 0:
        return None

    # âœ… æ£€æŸ¥æ‰¹æ¬¡æ˜¯å¦æœ‰æ•ˆ
    if not isinstance(batch[0], dict):
        warnings.warn(f"âš ï¸ Warning: Batch items are not dictionaries, skipping batch")
        return None

    if 'images' not in batch[0]:
        warnings.warn(f"âš ï¸ Warning: 'images' key not found in batch, skipping batch")
        return None

    # âœ… å®‰å…¨è·å–å›¾ç‰‡æ•°é‡
    try:
        num_images = []
        for item in batch:
            if 'images' not in item:
                warnings.warn(f"âš ï¸ Warning: 'images' missing in batch item, skipping batch")
                return None

            images = item['images']
            if not isinstance(images, torch.Tensor):
                warnings.warn(f"âš ï¸ Warning: 'images' is not a tensor, skipping batch")
                return None

            if images.ndim == 0:
                warnings.warn(f"âš ï¸ Warning: 'images' has no dimensions, skipping batch")
                return None

            num_images.append(images.shape[0])

        # âœ… æ£€æŸ¥å›¾ç‰‡æ•°é‡æ˜¯å¦ä¸€è‡´
        if len(set(num_images)) > 1:  # å¦‚æœå›¾ç‰‡æ•°é‡ä¸ä¸€è‡´
            min_num = min(num_images)
            warnings.warn(
                f"Batch has inconsistent number of images: {num_images}. "
                f"Cropping all to {min_num} images."
            )

            # âœ… åˆ›å»ºæ–°çš„ batchï¼ŒåŒ…å«è£å‰ªåçš„ tensor å‰¯æœ¬
            cropped_batch = []
            for item in batch:
                cropped_item = {}
                images_shape_0 = item['images'].shape[0]

                for key, value in item.items():
                    if isinstance(value, torch.Tensor):
                        # âœ… å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ tensor æœ‰ç»´åº¦
                        if value.ndim == 0:
                            # æ ‡é‡ tensorï¼ˆå¦‚ focal lengthï¼‰ï¼Œç›´æ¥å¤åˆ¶
                            cropped_item[key] = value.clone()
                        elif value.shape[0] == images_shape_0:
                            # ç¬¬ä¸€ç»´ä¸ images æ•°é‡ä¸€è‡´ï¼Œéœ€è¦è£å‰ª
                            cropped_item[key] = value[:min_num].contiguous().clone()
                        else:
                            # ç¬¬ä¸€ç»´ä¸ images æ•°é‡ä¸ä¸€è‡´ï¼Œä¸è£å‰ª
                            cropped_item[key] = value.contiguous().clone()
                    else:
                        # é tensor æ•°æ®ï¼ˆå¦‚å­—ç¬¦ä¸²ã€åˆ—è¡¨ç­‰ï¼‰ï¼Œç›´æ¥å¤åˆ¶
                        cropped_item[key] = value

                cropped_batch.append(cropped_item)

            batch = cropped_batch

    except Exception as e:
        warnings.warn(f"âš ï¸ Warning: Error during batch processing: {e}. Skipping batch.")
        import traceback
        traceback.print_exc()  # âœ… æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•
        return None

    # ä½¿ç”¨é»˜è®¤çš„ collate
    try:
        return torch.utils.data.dataloader.default_collate(batch)
    except RuntimeError as e:
        warnings.warn(f"âŒ Collate failed even after cropping: {e}. Skipping batch.")
        import traceback
        traceback.print_exc()
        return None


class Trainer:
    def __init__(
            self,
            net,
            train_dataset,
            val_dataset,
            args,
            conf,
            device=None,
            use_amp=False,  # âœ… æ·»åŠ æ··åˆç²¾åº¦å‚æ•°
    ):
        """
        åˆå§‹åŒ– Trainer

        Args:
            net: ç¥ç»ç½‘ç»œæ¨¡å‹
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            args: å‘½ä»¤è¡Œå‚æ•°
            conf: é…ç½®å­—å…¸
            device: è®­ç»ƒè®¾å¤‡
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.args = args
        self.net = net
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.device = device

        # âœ… æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
        self.use_amp = use_amp
        self.scaler = GradScaler() if use_amp else None

        # è®­ç»ƒå‚æ•°
        self.batch_size = args.batch_size
        self.num_epochs = conf.get_int("num_epochs", 100)
        self.lr = conf.get_float("lr", 1e-4)
        self.lr_policy = conf.get_string("lr_policy", "none")
        self.gamma = conf.get_float("gamma", 0.1)
        self.step_size = conf.get_int("step_size", 10000)

        # æ—¥å¿—å’Œä¿å­˜
        self.log_dir = os.path.join(args.logs_path, args.name)
        self.checkpoint_dir = os.path.join(args.checkpoints_path, args.name)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)

        self.writer = SummaryWriter(self.log_dir)
        self.print_interval = args.print_interval if hasattr(args, 'print_interval') else 10
        self.save_interval_epochs = conf.get_int("save_interval", 1)  # æ¯ 1 ä¸ª epoch ä¿å­˜
        self.eval_interval_epochs = conf.get_int("eval_interval", 10)  # æ¯ 10 ä¸ª epoch è¯„ä¼°
        self.vis_interval_epochs = conf.get_int("vis_interval", 10)  # æ¯ 10 ä¸ª epoch å¯è§†åŒ–

        # âœ… Checkpoint ç®¡ç†å‚æ•°
        self.keep_last_checkpoints = conf.get_int("keep_last_checkpoints", 20)  # ä¿ç•™æœ€è¿‘ 20 ä¸ª
        self.save_strategy = conf.get_string("save_strategy", "keep_last")  # keep_last, keep_all, milestone

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            [p for p in self.net.parameters() if p.requires_grad],
            lr=self.lr,
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.lr_policy == "step":
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.step_size,
                gamma=self.gamma,
            )
        elif self.lr_policy == "multistep":
            milestones = conf.get_list("milestones", [10000, 20000])
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=milestones,
                gamma=self.gamma,
            )
        else:
            self.scheduler = None

        # âœ… DataLoader ä¼˜åŒ–
        self.train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=False,  # âœ… å›ºå®šå†…å­˜åŠ é€Ÿ
            collate_fn=custom_collate_fn,
            drop_last=True,  # âœ… ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„ batch
        )

        self.val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=min(self.batch_size, 4),  # éªŒè¯æ—¶å¯ä»¥ç”¨æ›´å°çš„ batch
            shuffle=False,
            num_workers=0,  # âœ… éªŒè¯é›†ä¹ŸåŠ é€Ÿ
            pin_memory=False,
            collate_fn=custom_collate_fn,
        )

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.iter = 0
        self.global_step = 0
        self.best_val_loss = float('inf')

        if hasattr(args, 'resume') and args.resume:
            # å¦‚æœ resume æ˜¯å¸ƒå°”å€¼ Trueï¼Œè‡ªåŠ¨æŸ¥æ‰¾ latest.pth
            if isinstance(args.resume, bool):
                checkpoint_path = os.path.join(self.checkpoint_dir, "latest.pth")
                if os.path.exists(checkpoint_path):
                    print(f"âœ… Auto-resuming from: {checkpoint_path}")
                    self.load_checkpoint(checkpoint_path)
                else:
                    print(f"âš ï¸ No checkpoint found at {checkpoint_path}, starting from scratch")
            # å¦‚æœ resume æ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œç›´æ¥åŠ è½½
            elif isinstance(args.resume, str):
                if os.path.exists(args.resume):
                    print(f"âœ… Resuming from: {args.resume}")
                    self.load_checkpoint(args.resume)
                else:
                    raise FileNotFoundError(f"âŒ Checkpoint not found: {args.resume}")

    def post_batch(self, epoch, batch):
        """
        æ¯ä¸ª batch åçš„å›è°ƒï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰
        """
        pass

    def extra_save_state(self):
        """
        ä¿å­˜é¢å¤–çš„çŠ¶æ€ï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰
        """
        pass

    def train_step(self, data, global_step):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        raise NotImplementedError("Subclass must implement train_step")

    def eval_step(self, data, global_step):
        """
        å•ä¸ªéªŒè¯æ­¥éª¤ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        raise NotImplementedError("Subclass must implement eval_step")

    def vis_step(self, data, global_step, idx=None):
        """
        å¯è§†åŒ–æ­¥éª¤ï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°
            idx: æ‰¹æ¬¡ç´¢å¼•

        Returns:
            vis: å¯è§†åŒ–å›¾åƒ
            vals: æŒ‡æ ‡å­—å…¸
        """
        return None, {}

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch

        Args:
            epoch: å½“å‰ epoch ç¼–å·
        """
        self.net.train()

        epoch_loss = 0.0
        num_batches = 0

        # è¿›åº¦æ¡
        pbar = tqdm.tqdm(
            enumerate(self.train_loader),
            total=len(self.train_loader),
            desc=f"Epoch {epoch}",
        )

        iter_start_time = time.time()

        for batch_idx, data in pbar:
            if data is None:  # âœ… è·³è¿‡ç©ºæ‰¹æ¬¡
                continue

            # è®­ç»ƒæ­¥éª¤ï¼ˆå­ç±»å®ç°ï¼‰
            loss_dict = self.train_step(data, self.global_step)

            # ç´¯ç§¯æŸå¤±
            if "t" in loss_dict:
                epoch_loss += loss_dict["t"]
            elif "loss" in loss_dict:
                epoch_loss += loss_dict["loss"].item() if torch.is_tensor(loss_dict["loss"]) else loss_dict["loss"]
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            postfix_dict = {}
            for key, val in loss_dict.items():
                if key != "loss":
                    if torch.is_tensor(val):
                        postfix_dict[key] = f"{val.item():.4f}"
                    else:
                        postfix_dict[key] = f"{val:.4f}"
            postfix_dict["lr"] = f"{self.optimizer.param_groups[0]['lr']:.6f}"
            pbar.set_postfix(postfix_dict)

            # æ‰“å°å’Œè®°å½•
            if batch_idx % self.print_interval == 0:
                iter_time = time.time() - iter_start_time

                log_str = f"[{iter_time:.2f}s/it] E {epoch} B {batch_idx}"
                for key, val in loss_dict.items():
                    if key != "loss":
                        if torch.is_tensor(val):
                            log_str += f" {key}:{val.item():.4f}"
                        else:
                            log_str += f" {key}:{val:.4f}"
                log_str += f" lr {self.optimizer.param_groups[0]['lr']:.6f}"
                print(log_str)

                # è®°å½•åˆ° tensorboard
                for key, val in loss_dict.items():
                    if key != "loss":
                        if torch.is_tensor(val):
                            self.writer.add_scalar(f"train/{key}", val.item(), self.global_step)
                        else:
                            self.writer.add_scalar(f"train/{key}", val, self.global_step)
                self.writer.add_scalar("train/lr", self.optimizer.param_groups[0]['lr'], self.global_step)

                iter_start_time = time.time()

            # æ¯ä¸ª batch åçš„å›è°ƒ
            self.post_batch(epoch, batch_idx)

            self.global_step += 1

        # Epoch ç»Ÿè®¡
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0

        print(f"Epoch {epoch} finished: avg_loss={avg_loss:.4f}")

        # âœ… æŒ‰ epoch ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¿®æ”¹åçš„é€»è¾‘ï¼‰
        if (epoch + 1) % self.save_interval_epochs == 0:
            self.save_checkpoint_with_epoch(epoch, avg_loss)

        # âœ… æŒ‰ epoch éªŒè¯
        if (epoch + 1) % self.eval_interval_epochs == 0:
            print(f"\n{'=' * 80}")
            print(f"ğŸ“Š Evaluation at epoch {epoch}")
            print(f"{'=' * 80}")
            val_loss = self.validate()

            # âœ… å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜ best.pth
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint("best.pth", is_best=True)

        # âœ… æŒ‰ epoch å¯è§†åŒ–
        if (epoch + 1) % self.vis_interval_epochs == 0 and len(self.val_loader) > 0:
            # ä»éªŒè¯é›†ä¸­å–ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
            try:
                val_data = next(iter(self.val_loader))
                if val_data is not None:
                    vis, vals = self.vis_step(val_data, self.global_step)
                    if vis is not None:
                        self.writer.add_image("vis", vis, self.global_step, dataformats='HWC')
                    for key, val in vals.items():
                        self.writer.add_scalar(f"vis/{key}", val, self.global_step)
            except Exception as e:
                print(f"âš ï¸ Visualization failed: {e}")

        # æ›´æ–°å­¦ä¹ ç‡
        if self.scheduler is not None:
            self.scheduler.step()

        return avg_loss

    def validate(self):
        """
        éªŒè¯æ¨¡å‹

        Returns:
            avg_val_loss: å¹³å‡éªŒè¯æŸå¤±
        """
        self.net.eval()

        val_loss = 0.0
        num_batches = 0
        num_skipped = 0  # âœ… ç»Ÿè®¡è·³è¿‡çš„æ‰¹æ¬¡

        with torch.no_grad():
            for data in tqdm.tqdm(self.val_loader, desc="Validation"):
                # âœ… è·³è¿‡ None æ‰¹æ¬¡ï¼ˆcollate å¤±è´¥ï¼‰
                if data is None:
                    num_skipped += 1
                    continue

                # âœ… è·³è¿‡ç©ºæ‰¹æ¬¡
                if not data or 'images' not in data:
                    num_skipped += 1
                    continue

                try:
                    # éªŒè¯æ­¥éª¤ï¼ˆå­ç±»å®ç°ï¼‰
                    loss_dict = self.eval_step(data, self.global_step)

                    if "t" in loss_dict:
                        val_loss += loss_dict["t"]
                    elif "loss" in loss_dict:
                        val_loss += loss_dict["loss"].item() if torch.is_tensor(loss_dict["loss"]) else loss_dict[
                            "loss"]
                    num_batches += 1

                except RuntimeError as e:
                    print(f"\nâš ï¸ Skipping validation batch due to error: {e}")
                    num_skipped += 1
                    continue

        # âœ… è®¡ç®—å¹³å‡æŸå¤±
        if num_batches > 0:
            avg_val_loss = val_loss / num_batches
            if num_skipped > 0:
                print(f"\nâš ï¸ Validation: Skipped {num_skipped} problematic batches")
        else:
            print("\nâš ï¸ No valid validation batches!")
            avg_val_loss = float('inf')

        print(f"Validation: avg_loss={avg_val_loss:.4f}")

        # è®°å½•åˆ° tensorboard
        self.writer.add_scalar("val/loss", avg_val_loss, self.global_step)

        self.net.train()
        return avg_val_loss

    # âœ… ============================================================
    # âœ… æ–°å¢ï¼šå¸¦ epoch ç¼–å·çš„ checkpoint ä¿å­˜ï¼ˆä¸è¦†ç›–ï¼‰
    # âœ… ============================================================
    def save_checkpoint_with_epoch(self, epoch, train_loss=None):
        """
        ä¿å­˜å¸¦ epoch ç¼–å·çš„ checkpointï¼ˆä¸è¦†ç›–ä¹‹å‰çš„ï¼‰

        Args:
            epoch: å½“å‰ epoch ç¼–å·
            train_loss: è®­ç»ƒæŸå¤±ï¼ˆå¯é€‰ï¼‰
        """
        # 1. ä¿å­˜å¸¦ epoch ç¼–å·çš„ checkpointï¼ˆä¿å­˜å½“å‰ epochï¼‰
        epoch_filename = f"epoch_{epoch:04d}.pth"
        self.save_checkpoint(epoch_filename, is_best=False, save_epoch=epoch)

        # 2. âœ… ä¿å­˜ä¸º latest.pth æ—¶ï¼Œä¿å­˜ä¸‹ä¸€ä¸ª epochï¼ˆç”¨äºæ¢å¤ï¼‰
        self.save_checkpoint("latest.pth", is_best=False, save_epoch=epoch + 1)

        # 3. æ ¹æ®ä¿å­˜ç­–ç•¥æ¸…ç†æ—§ checkpoint
        if self.save_strategy == "keep_last":
            self.cleanup_old_checkpoints(keep_last=self.keep_last_checkpoints)
        elif self.save_strategy == "milestone":
            self.cleanup_milestone_checkpoints(epoch)
        # keep_all ç­–ç•¥ä¸æ¸…ç†

        # 4. æ˜¾ç¤ºç£ç›˜ä½¿ç”¨æƒ…å†µ
        self.print_checkpoint_disk_usage()

    # âœ… ============================================================
    # âœ… æ–°å¢ï¼šæ¸…ç†æ—§ checkpointï¼ˆåªä¿ç•™æœ€è¿‘ N ä¸ªï¼‰
    # âœ… ============================================================
    def cleanup_old_checkpoints(self, keep_last=20):
        """
        æ¸…ç†æ—§çš„ checkpointï¼Œåªä¿ç•™æœ€è¿‘çš„ N ä¸ª

        Args:
            keep_last: ä¿ç•™æœ€è¿‘çš„ N ä¸ª checkpoint
        """
        # è·å–æ‰€æœ‰å¸¦ epoch ç¼–å·çš„ checkpoint
        pattern = os.path.join(self.checkpoint_dir, "epoch_*.pth")
        checkpoints = sorted(glob.glob(pattern))

        # åˆ é™¤æ—§çš„ checkpoint
        if len(checkpoints) > keep_last:
            num_to_delete = len(checkpoints) - keep_last
            for old_checkpoint in checkpoints[:num_to_delete]:
                try:
                    os.remove(old_checkpoint)
                    print(f"ğŸ—‘ï¸  Removed old checkpoint: {os.path.basename(old_checkpoint)}")
                except Exception as e:
                    print(f"âš ï¸  Failed to remove {os.path.basename(old_checkpoint)}: {e}")

    # âœ… ============================================================
    # âœ… æ–°å¢ï¼šé‡Œç¨‹ç¢‘å¼ä¿å­˜ç­–ç•¥
    # âœ… ============================================================
    def cleanup_milestone_checkpoints(self, current_epoch):
        """
        é‡Œç¨‹ç¢‘å¼ä¿å­˜ç­–ç•¥ï¼š
        - å‰ 10 ä¸ª epochï¼šå…¨éƒ¨ä¿ç•™
        - 10-100 epochï¼šæ¯ 5 ä¸ªä¿ç•™ä¸€ä¸ª
        - 100+ epochï¼šæ¯ 20 ä¸ªä¿ç•™ä¸€ä¸ª

        Args:
            current_epoch: å½“å‰ epoch
        """
        pattern = os.path.join(self.checkpoint_dir, "epoch_*.pth")
        checkpoints = sorted(glob.glob(pattern))

        for checkpoint_path in checkpoints:
            # æå– epoch ç¼–å·
            basename = os.path.basename(checkpoint_path)
            try:
                epoch_num = int(basename.split('_')[1].split('.')[0])
            except:
                continue

            # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™
            should_keep = (
                    epoch_num <= 10 or  # å‰ 10 ä¸ªå…¨éƒ¨ä¿ç•™
                    (epoch_num <= 100 and epoch_num % 5 == 0) or  # 10-100 æ¯ 5 ä¸ªä¿ç•™
                    (epoch_num > 100 and epoch_num % 20 == 0) or  # 100+ æ¯ 20 ä¸ªä¿ç•™
                    epoch_num == current_epoch  # å½“å‰ epoch ä¿ç•™
            )

            if not should_keep:
                try:
                    os.remove(checkpoint_path)
                    print(f"ğŸ—‘ï¸  Removed checkpoint: {basename}")
                except Exception as e:
                    print(f"âš ï¸  Failed to remove {basename}: {e}")

    # âœ… ============================================================
    # âœ… æ–°å¢ï¼šæ˜¾ç¤º checkpoint ç£ç›˜ä½¿ç”¨æƒ…å†µ
    # âœ… ============================================================
    def print_checkpoint_disk_usage(self):
        """
        æ‰“å° checkpoint ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
        """
        try:
            pattern = os.path.join(self.checkpoint_dir, "*.pth")
            checkpoints = glob.glob(pattern)

            total_size = 0
            for checkpoint in checkpoints:
                total_size += os.path.getsize(checkpoint)

            total_size_mb = total_size / (1024 * 1024)
            total_size_gb = total_size / (1024 * 1024 * 1024)

            if total_size_gb > 1:
                print(f"ğŸ’¾ Checkpoint disk usage: {total_size_gb:.2f} GB ({len(checkpoints)} files)")
            else:
                print(f"ğŸ’¾ Checkpoint disk usage: {total_size_mb:.2f} MB ({len(checkpoints)} files)")
        except Exception as e:
            print(f"âš ï¸  Failed to calculate disk usage: {e}")

    # âœ… ============================================================
    # âœ… ä¿®æ”¹ï¼šåŸæœ‰çš„ save_checkpoint å‡½æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    # âœ… ============================================================
    def save_checkpoint(self, filename, is_best=False, save_epoch=None):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹

        Args:
            filename: æ–‡ä»¶å
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            save_epoch: ä¿å­˜çš„ epoch ç¼–å·ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨ self.epochï¼‰
        """
        # âœ… å¦‚æœæŒ‡å®šäº† save_epochï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨ self.epoch
        epoch_to_save = save_epoch if save_epoch is not None else self.epoch

        checkpoint = {
            "epoch": epoch_to_save,  # âœ… ä½¿ç”¨æŒ‡å®šçš„ epoch
            "iter": self.iter,
            "global_step": self.global_step,
            "net_state_dict": self.net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "best_val_loss": self.best_val_loss,
        }

        if self.scheduler is not None:
            checkpoint["scheduler_state_dict"] = self.scheduler.state_dict()

        # âœ… ä¿å­˜ scaler çŠ¶æ€ï¼ˆç”¨äºæ¢å¤æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if self.use_amp and self.scaler is not None:
            checkpoint["scaler_state_dict"] = self.scaler.state_dict()

        filepath = os.path.join(self.checkpoint_dir, filename)
        torch.save(checkpoint, filepath)

        if is_best:
            print(f"ğŸŒŸ Best checkpoint saved: {filename} (loss: {self.best_val_loss:.4f})")
        elif filename != "latest.pth":
            print(f"ğŸ’¾ Checkpoint saved: {filename}")

        # è°ƒç”¨å­ç±»çš„é¢å¤–ä¿å­˜é€»è¾‘
        self.extra_save_state()

    def load_checkpoint(self, filepath):
        """
        åŠ è½½æ£€æŸ¥ç‚¹

        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(filepath):
            print(f"âŒ Checkpoint not found: {filepath}")
            return

        checkpoint = torch.load(filepath, map_location=self.device)

        self.epoch = checkpoint.get("epoch", 0)
        self.iter = checkpoint.get("iter", 0)
        self.global_step = checkpoint.get("global_step", 0)
        self.best_val_loss = checkpoint.get("best_val_loss", float('inf'))

        self.net.load_state_dict(checkpoint["net_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        if self.scheduler is not None and "scheduler_state_dict" in checkpoint:
            self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

        # âœ… æ¢å¤ scaler çŠ¶æ€
        if self.use_amp and self.scaler is not None and "scaler_state_dict" in checkpoint:
            self.scaler.load_state_dict(checkpoint["scaler_state_dict"])

        # âœ… éªŒè¯ epoch å’Œ global_step çš„ä¸€è‡´æ€§
        if len(self.train_loader) > 0:
            batches_per_epoch = len(self.train_loader)
            expected_epoch = self.global_step // batches_per_epoch

            if expected_epoch != self.epoch:
                print(f"\nâš ï¸  Checkpoint inconsistency detected!")
                print(f"   Saved epoch: {self.epoch}")
                print(f"   Global step: {self.global_step}")
                print(f"   Expected epoch (from global_step): {expected_epoch}")
                print(f"   Batches per epoch: {batches_per_epoch}")

                # âœ… è‡ªåŠ¨ä¿®æ­£ï¼ˆä½¿ç”¨ global_step è®¡ç®—çš„ epochï¼‰
                if "latest.pth" in filepath:
                    print(f"   ğŸ”§ Auto-correcting to epoch {expected_epoch}")
                    self.epoch = expected_epoch
                else:
                    print(f"   âš ï¸  Using saved epoch {self.epoch} (not latest.pth)")

        print(f"âœ… Checkpoint loaded: {filepath}")
        print(f"ğŸ“ Resuming from epoch {self.epoch}, global_step {self.global_step}")

    def start(self):
        """
        å¼€å§‹è®­ç»ƒ
        """
        print("=" * 80)
        print(f"ğŸš€ Training started: {self.args.name}")
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        print(f"ğŸ“¦ Batch size: {self.batch_size}")
        print(f"ğŸ“ˆ Learning rate: {self.lr}")
        print(f"ğŸ”„ Num epochs: {self.num_epochs}")
        print(f"âš¡ Mixed Precision: {self.use_amp}")
        print(f"ğŸ’¾ Checkpoints: {self.checkpoint_dir}")
        print(f"ğŸ“Š Logs: {self.log_dir}")
        print(f"ğŸ—‚ï¸  Save strategy: {self.save_strategy}")
        if self.save_strategy == "keep_last":
            print(f"ğŸ“ Keep last {self.keep_last_checkpoints} checkpoints")
        print("=" * 80)

        # âœ… æ–°å¢ï¼šå¦‚æœæ¢å¤çš„ epoch æ˜¯è¯„ä¼°èŠ‚ç‚¹ï¼Œå…ˆè¯„ä¼°ä¸€æ¬¡
        if self.epoch > 0 and self.epoch % self.eval_interval_epochs == 0:
            print(f"\n{'=' * 80}")
            print(f"ğŸ“Š Running evaluation for resumed epoch {self.epoch}")
            print(f"{'=' * 80}")

            val_loss = self.validate()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint("best.pth", is_best=True)

            print(f"{'=' * 80}\n")

        # ç»§ç»­æ­£å¸¸è®­ç»ƒ
        for epoch in range(self.epoch, self.num_epochs):
            self.epoch = epoch

            # è®­ç»ƒä¸€ä¸ª epoch
            avg_loss = self.train_epoch(epoch)

        print("=" * 80)
        print("ğŸ‰ Training finished!")
        print("=" * 80)

        self.writer.close()
