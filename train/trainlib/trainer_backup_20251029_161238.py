import os.path
import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import tqdm
import warnings
import time

# âœ… æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
from torch.cuda.amp import autocast, GradScaler


def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰ collate å‡½æ•°ï¼Œå¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    1. è¿‡æ»¤æ‰ None æ ·æœ¬
    2. å¤„ç†ä¸åŒæ•°é‡å›¾ç‰‡çš„æ ·æœ¬ï¼ˆç»Ÿä¸€è£å‰ªæ‰€æœ‰ç›¸å…³å­—æ®µï¼‰
    """
    # è¿‡æ»¤æ‰ Noneï¼ˆè·³è¿‡çš„æ ·æœ¬ï¼‰
    batch = [item for item in batch if item is not None]

    if len(batch) == 0:
        return None

    # âœ… æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬çš„å›¾ç‰‡æ•°é‡ä¸€è‡´
    if isinstance(batch[0], dict) and 'images' in batch[0]:
        num_images = [item['images'].shape[0] for item in batch]

        if len(set(num_images)) > 1:  # å¦‚æœå›¾ç‰‡æ•°é‡ä¸ä¸€è‡´
            min_num = min(num_images)
            warnings.warn(
                f"Batch has inconsistent number of images: {num_images}. "
                f"Cropping all to {min_num} images."
            )

            # è£å‰ªæ‰€æœ‰æ ·æœ¬åˆ°æœ€å°å›¾ç‰‡æ•°
            for item in batch:
                if item['images'].shape[0] > min_num:
                    item['images'] = item['images'][:min_num]
                    item['poses'] = item['poses'][:min_num]

                    # å¦‚æœæœ‰å…¶ä»–ç›¸å…³å­—æ®µä¹Ÿéœ€è¦è£å‰ª
                    if 'all_rays' in item:
                        item['all_rays'] = item['all_rays'][:min_num]
                    if 'all_rgb' in item:
                        item['all_rgb'] = item['all_rgb'][:min_num]

    # ä½¿ç”¨é»˜è®¤çš„ collate
    return torch.utils.data.dataloader.default_collate(batch)


class Trainer:
    def __init__(
            self,
            net,
            train_dataset,
            val_dataset,
            args,
            conf,
            device=None,
            use_amp=False,  # âœ… æ·»åŠ æ··åˆç²¾åº¦å‚æ•°
    ):
        """
        åˆå§‹åŒ– Trainer

        Args:
            net: ç¥ç»ç½‘ç»œæ¨¡å‹
            train_dataset: è®­ç»ƒæ•°æ®é›†
            val_dataset: éªŒè¯æ•°æ®é›†
            args: å‘½ä»¤è¡Œå‚æ•°
            conf: é…ç½®å­—å…¸
            device: è®­ç»ƒè®¾å¤‡
            use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        """
        self.args = args
        self.net = net
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.device = device

        # âœ… æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
        self.use_amp = use_amp
        self.scaler = GradScaler() if use_amp else None

        # è®­ç»ƒå‚æ•°
        self.batch_size = args.batch_size
        self.num_epochs = conf.get_int("num_epochs", 100000)
        self.lr = conf.get_float("lr", 1e-4)
        self.lr_policy = conf.get_string("lr_policy", "none")
        self.gamma = conf.get_float("gamma", 0.1)
        self.step_size = conf.get_int("step_size", 10000)

        # æ—¥å¿—å’Œä¿å­˜
        self.log_dir = os.path.join(args.logs_path, args.name)
        self.checkpoint_dir = os.path.join(args.checkpoints_path, args.name)
        os.makedirs(self.log_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)

        self.writer = SummaryWriter(self.log_dir)
        self.print_interval = args.print_interval if hasattr(args, 'print_interval') else 10
        self.save_interval_epochs = conf.get_int("save_interval", 1)  # æ¯ 1 ä¸ª epoch ä¿å­˜
        self.eval_interval_epochs = conf.get_int("eval_interval", 10)  # æ¯ 10 ä¸ª epoch è¯„ä¼°
        self.vis_interval_epochs = conf.get_int("vis_interval", 10)  # æ¯ 10 ä¸ª epoch å¯è§†åŒ–

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            [p for p in self.net.parameters() if p.requires_grad],
            lr=self.lr,
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.lr_policy == "step":
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.step_size,
                gamma=self.gamma,
            )
        elif self.lr_policy == "multistep":
            milestones = conf.get_list("milestones", [10000, 20000])
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=milestones,
                gamma=self.gamma,
            )
        else:
            self.scheduler = None

        # âœ… DataLoader ä¼˜åŒ–
        self.train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=True,  # âœ… å›ºå®šå†…å­˜åŠ é€Ÿ
            collate_fn=custom_collate_fn,
            drop_last=True,  # âœ… ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„ batch
        )

        self.val_loader = torch.utils.data.DataLoader(
            val_dataset,
            batch_size=min(self.batch_size, 4),  # éªŒè¯æ—¶å¯ä»¥ç”¨æ›´å°çš„ batch
            shuffle=False,
            num_workers=2,  # âœ… éªŒè¯é›†ä¹ŸåŠ é€Ÿ
            pin_memory=True,
            collate_fn=custom_collate_fn,
        )

        # è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.iter = 0
        self.global_step = 0
        self.best_val_loss = float('inf')

        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(args, 'resume') and args.resume:
            self.load_checkpoint(args.resume)

    def post_batch(self, epoch, batch):
        """
        æ¯ä¸ª batch åçš„å›è°ƒï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰
        """
        pass

    def extra_save_state(self):
        """
        ä¿å­˜é¢å¤–çš„çŠ¶æ€ï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰
        """
        pass

    def train_step(self, data, global_step):
        """
        å•ä¸ªè®­ç»ƒæ­¥éª¤ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        raise NotImplementedError("Subclass must implement train_step")

    def eval_step(self, data, global_step):
        """
        å•ä¸ªéªŒè¯æ­¥éª¤ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°

        Returns:
            loss_dict: æŸå¤±å­—å…¸
        """
        raise NotImplementedError("Subclass must implement eval_step")

    def vis_step(self, data, global_step, idx=None):
        """
        å¯è§†åŒ–æ­¥éª¤ï¼ˆå­ç±»å¯ä»¥é‡å†™ï¼‰

        Args:
            data: æ‰¹æ¬¡æ•°æ®
            global_step: å…¨å±€æ­¥æ•°
            idx: æ‰¹æ¬¡ç´¢å¼•

        Returns:
            vis: å¯è§†åŒ–å›¾åƒ
            vals: æŒ‡æ ‡å­—å…¸
        """
        return None, {}

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch

        Args:
            epoch: å½“å‰ epoch ç¼–å·
        """
        self.net.train()

        epoch_loss = 0.0
        num_batches = 0

        # è¿›åº¦æ¡
        pbar = tqdm.tqdm(
            enumerate(self.train_loader),
            total=len(self.train_loader),
            desc=f"Epoch {epoch}",
        )

        iter_start_time = time.time()

        for batch_idx, data in pbar:
            if data is None:  # è·³è¿‡ç©ºæ‰¹æ¬¡
                continue

            # è®­ç»ƒæ­¥éª¤ï¼ˆå­ç±»å®ç°ï¼‰
            loss_dict = self.train_step(data, self.global_step)

            # ç´¯ç§¯æŸå¤±
            if "t" in loss_dict:
                epoch_loss += loss_dict["t"]
            elif "loss" in loss_dict:
                epoch_loss += loss_dict["loss"].item() if torch.is_tensor(loss_dict["loss"]) else loss_dict["loss"]
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            postfix_dict = {}
            for key, val in loss_dict.items():
                if key != "loss":
                    if torch.is_tensor(val):
                        postfix_dict[key] = f"{val.item():.4f}"
                    else:
                        postfix_dict[key] = f"{val:.4f}"
            postfix_dict["lr"] = f"{self.optimizer.param_groups[0]['lr']:.6f}"
            pbar.set_postfix(postfix_dict)

            # æ‰“å°å’Œè®°å½•
            if batch_idx % self.print_interval == 0:
                iter_time = time.time() - iter_start_time

                log_str = f"[{iter_time:.2f}s/it] E {epoch} B {batch_idx}"
                for key, val in loss_dict.items():
                    if key != "loss":
                        if torch.is_tensor(val):
                            log_str += f" {key}:{val.item():.4f}"
                        else:
                            log_str += f" {key}:{val:.4f}"
                log_str += f" lr {self.optimizer.param_groups[0]['lr']:.6f}"
                print(log_str)

                # è®°å½•åˆ° tensorboard
                for key, val in loss_dict.items():
                    if key != "loss":
                        if torch.is_tensor(val):
                            self.writer.add_scalar(f"train/{key}", val.item(), self.global_step)
                        else:
                            self.writer.add_scalar(f"train/{key}", val, self.global_step)
                self.writer.add_scalar("train/lr", self.optimizer.param_groups[0]['lr'], self.global_step)

                iter_start_time = time.time()

            # âœ… ä¿å­˜æ£€æŸ¥ç‚¹é€»è¾‘å·²ç§»åˆ° epoch ç»“æŸå
            # if self.global_step % self.save_interval == 0 and self.global_step > 0:
            #     self.save_checkpoint(f"iter_{self.global_step}.pth")

            # âœ… éªŒè¯é€»è¾‘å·²ç§»åˆ° epoch ç»“æŸå
            # if self.global_step % self.eval_interval == 0 and self.global_step > 0:
            #     self.validate()

            # âœ… å¯è§†åŒ–é€»è¾‘å·²ç§»åˆ° epoch ç»“æŸå
            # if self.global_step % self.vis_interval == 0 and self.global_step > 0:
            #     vis, vals = self.vis_step(data, self.global_step)
            #     if vis is not None:
            #         self.writer.add_image("vis", vis, self.global_step, dataformats='HWC')
            #     for key, val in vals.items():
            #         self.writer.add_scalar(f"vis/{key}", val, self.global_step)

            # æ¯ä¸ª batch åçš„å›è°ƒ
            self.post_batch(epoch, batch_idx)

            self.global_step += 1

        # Epoch ç»Ÿè®¡
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0

        print(f"Epoch {epoch} finished: avg_loss={avg_loss:.4f}")

        # âœ… æŒ‰ epoch ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % self.save_interval_epochs == 0:
            self.save_checkpoint(f"epoch_{epoch}.pth")
            print(f"ğŸ’¾ Epoch {epoch} checkpoint saved")

        # âœ… æŒ‰ epoch éªŒè¯
        if (epoch + 1) % self.eval_interval_epochs == 0:
            print(f"\n{'=' * 80}")
            print(f"ğŸ“Š Evaluation at epoch {epoch}")
            print(f"{'=' * 80}")
            self.validate()

        # âœ… æŒ‰ epoch å¯è§†åŒ–
        if (epoch + 1) % self.vis_interval_epochs == 0 and len(self.val_loader) > 0:
            # ä»éªŒè¯é›†ä¸­å–ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
            try:
                val_data = next(iter(self.val_loader))
                if val_data is not None:
                    vis, vals = self.vis_step(val_data, self.global_step)
                    if vis is not None:
                        self.writer.add_image("vis", vis, self.global_step, dataformats='HWC')
                    for key, val in vals.items():
                        self.writer.add_scalar(f"vis/{key}", val, self.global_step)
            except Exception as e:
                print(f"âš ï¸ Visualization failed: {e}")

        # æ›´æ–°å­¦ä¹ ç‡
        if self.scheduler is not None:
            self.scheduler.step()

        return avg_loss

    def validate(self):
        """
        éªŒè¯æ¨¡å‹

        Returns:
            avg_val_loss: å¹³å‡éªŒè¯æŸå¤±
        """
        self.net.eval()

        val_loss = 0.0
        num_batches = 0

        with torch.no_grad():
            for data in tqdm.tqdm(self.val_loader, desc="Validation"):
                if data is None:
                    continue

                # éªŒè¯æ­¥éª¤ï¼ˆå­ç±»å®ç°ï¼‰
                loss_dict = self.eval_step(data, self.global_step)

                if "t" in loss_dict:
                    val_loss += loss_dict["t"]
                elif "loss" in loss_dict:
                    val_loss += loss_dict["loss"].item() if torch.is_tensor(loss_dict["loss"]) else loss_dict["loss"]
                num_batches += 1

        avg_val_loss = val_loss / num_batches if num_batches > 0 else 0

        print(f"Validation: avg_loss={avg_val_loss:.4f}")

        # è®°å½•åˆ° tensorboard
        self.writer.add_scalar("val/loss", avg_val_loss, self.global_step)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < self.best_val_loss:
            self.best_val_loss = avg_val_loss
            self.save_checkpoint("best.pth")
            print(f"âœ… New best model saved! Loss: {avg_val_loss:.4f}")

        self.net.train()
        return avg_val_loss

    def save_checkpoint(self, filename):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹

        Args:
            filename: æ–‡ä»¶å
        """
        checkpoint = {
            "epoch": self.epoch,
            "iter": self.iter,
            "global_step": self.global_step,
            "net_state_dict": self.net.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "best_val_loss": self.best_val_loss,
        }

        if self.scheduler is not None:
            checkpoint["scheduler_state_dict"] = self.scheduler.state_dict()

        # âœ… ä¿å­˜ scaler çŠ¶æ€ï¼ˆç”¨äºæ¢å¤æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        if self.use_amp and self.scaler is not None:
            checkpoint["scaler_state_dict"] = self.scaler.state_dict()

        filepath = os.path.join(self.checkpoint_dir, filename)
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filepath}")

        # è°ƒç”¨å­ç±»çš„é¢å¤–ä¿å­˜é€»è¾‘
        self.extra_save_state()

    def load_checkpoint(self, filepath):
        """
        åŠ è½½æ£€æŸ¥ç‚¹

        Args:
            filepath: æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(filepath):
            print(f"âŒ Checkpoint not found: {filepath}")
            return

        checkpoint = torch.load(filepath, map_location=self.device)

        self.epoch = checkpoint.get("epoch", 0)
        self.iter = checkpoint.get("iter", 0)
        self.global_step = checkpoint.get("global_step", 0)
        self.best_val_loss = checkpoint.get("best_val_loss", float('inf'))

        self.net.load_state_dict(checkpoint["net_state_dict"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        if self.scheduler is not None and "scheduler_state_dict" in checkpoint:
            self.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

        # âœ… æ¢å¤ scaler çŠ¶æ€
        if self.use_amp and self.scaler is not None and "scaler_state_dict" in checkpoint:
            self.scaler.load_state_dict(checkpoint["scaler_state_dict"])

        print(f"âœ… Checkpoint loaded: {filepath}")
        print(f"ğŸ“ Resuming from epoch {self.epoch}, global_step {self.global_step}")

    def start(self):
        """
        å¼€å§‹è®­ç»ƒ
        """
        print("=" * 80)
        print(f"ğŸš€ Training started: {self.args.name}")
        print(f"ğŸ–¥ï¸  Device: {self.device}")
        print(f"ğŸ“¦ Batch size: {self.batch_size}")
        print(f"ğŸ“ˆ Learning rate: {self.lr}")
        print(f"ğŸ”„ Num epochs: {self.num_epochs}")
        print(f"âš¡ Mixed Precision: {self.use_amp}")
        print(f"ğŸ’¾ Checkpoints: {self.checkpoint_dir}")
        print(f"ğŸ“Š Logs: {self.log_dir}")
        print("=" * 80)

        for epoch in range(self.epoch, self.num_epochs):
            self.epoch = epoch

            # è®­ç»ƒä¸€ä¸ª epoch
            avg_loss = self.train_epoch(epoch)


        print("=" * 80)
        print("ğŸ‰ Training finished!")
        print("=" * 80)

        self.writer.close()
